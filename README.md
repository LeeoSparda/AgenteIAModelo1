# AgenteIAModelo1
ğŸ“˜ README â€“ ExecuÃ§Ã£o de Chat Local e API com Modelos Hugging Face

Este projeto contÃ©m um script completo (main.py) capaz de:

Instalar automaticamente todas as dependÃªncias necessÃ¡rias

Detectar e configurar GPU com PyTorch CUDA 12.1

Carregar e executar um chat local com modelos Hugging Face

Alternativamente, executar um chat consumindo a API da Hugging Face

Suportar mÃºltiplos modelos (texto, OCR, document-QA, visÃ£o, etc.) dependendo do pipeline ativado

Este manual explica tudo o que precisa ser feito antes de rodar a aplicaÃ§Ã£o, incluindo instalaÃ§Ã£o, configuraÃ§Ã£o e execuÃ§Ã£o.

ğŸ§© 1. Requisitos do Sistema
âœ” Python

Python 3.9 ou superior

âœ” Hardware (opcional, mas recomendado)

GPU NVIDIA com CUDA 12.1 para acelerar modelos modernos

Se nÃ£o tiver GPU, o script funciona em CPU, apenas mais lento

âœ” Sistemas compatÃ­veis

Windows 10/11

Linux (altamente recomendado)

macOS (funciona apenas em CPU)

ğŸ“ 2. Estrutura Recomendada do Projeto
/seu-projeto
â”‚
â”œâ”€â”€ main.py               â†’ script principal (chat local + API)
â””â”€â”€ README.md             â†’ este manual

ğŸ§ª 3. Criar Ambiente Virtual
Windows
python -m venv venv
venv\Scripts\activate

Linux / macOS
python3 -m venv venv
source venv/bin/activate

ğŸ“¦ 4. Instalar DependÃªncias (o script jÃ¡ instala sozinho)

O script instala automaticamente:

torch

transformers

accelerate

sentencepiece

requests

Mas, caso queira instalar manualmente:

pip install torch transformers accelerate sentencepiece requests

ğŸ”¥ Instalar PyTorch com CUDA 12.1 (para RTX 4060, 4070 etc.)

Se quiser instalar manualmente:

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121


O prÃ³prio script tambÃ©m realiza esta operaÃ§Ã£o caso detecte erro no PyTorch.

ğŸ”‘ 5. Token Hugging Face

O script utiliza um token configurado diretamente:

get_token = "hf_xxxxxxxx"
os.environ["HF_TOKEN"] = get_token


Para utilizar seu prÃ³prio token:

Crie um token em:
https://huggingface.co/settings/tokens

Substitua a variÃ¡vel get_token.

ğŸ¤– 6. Escolher o Modelo para Carregar

O script possui vÃ¡rias opÃ§Ãµes de modelos jÃ¡ configuradas.
Basta descomentar o modelo desejado.

Exemplos:

Modelo de texto (Phi-3-mini)
model_name = "microsoft/Phi-3-mini-4k-instruct"

OCR (Trocr)
model_name = "microsoft/trocr-base-stage1"

Document-QA (LayoutLMv3)
model_name = "microsoft/layoutlmv3-base"

VisÃ£o multimodal (SmolVLM)
model_name = "HuggingFaceM4/smolvlm-instruct"


ApÃ³s escolher, o pipeline Ã© configurado automaticamente.

ğŸ’¬ 7. Chat Local â€“ Modo Offline

Para iniciar o chat local:

python main.py


O script irÃ¡:

Instalar dependÃªncias

Detectar GPU

Baixar o modelo

Criar o pipeline

Abrir o chat no terminal

Uso

Digite mensagens:

VocÃª: OlÃ¡!
Assistente: OlÃ¡! Como posso ajudar?


Para sair:

fim

ğŸŒ 8. Alternativa: Chat via API Hugging Face

ApÃ³s o chat local, o script inicia o modo API, usando:

https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct

Como funciona

O script envia requisiÃ§Ãµes para a API oficial da HF:

response = requests.post(API_URL, headers=headers, json=payload)


E exibe a resposta no terminal.

Uso
VocÃª: Explique o que Ã© machine learning
Assistente: ...


Para sair:

fim

âš ï¸ 9. PossÃ­veis Problemas e SoluÃ§Ãµes
âŒ GPU nÃ£o detectada

Verifique drivers NVIDIA

Verifique versÃ£o CUDA

Tente reinstalar PyTorch com CUDA 12.1

âŒ â€œCUDA error: ...â€

Instalar PyTorch compatÃ­vel com sua GPU:

pip install torch --index-url https://download.pytorch.org/whl/cu121

âŒ Token invÃ¡lido

Certifique-se de que:

O token Ã© do tipo read

EstÃ¡ ativo

EstÃ¡ corretamente configurado no script

âŒ Modelo muito grande para GPU

Escolha modelos menores:

microsoft/Phi-3-mini-4k-instruct

Qwen/Qwen2.5-0.5B

google/gemma-2b

ğŸ“„ 10. LicenÃ§a

Recomenda-se utilizar MIT License, mas pode ajustar conforme necessidade.
